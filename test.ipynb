{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification and Profanity Detection\n",
    "\n",
    "This notebook outlines the process for building a system that can identify languages and detect profanity in sentences. It consists of three main components:\n",
    "1. **FNLI.py**: This module is responsible for generating dictionaries and training the language identification model.\n",
    "2. **POSTagger.py**: This module implements the Part-of-Speech (POS) tagging using the Stanford POS Tagger.\n",
    "3. **TAKLUBAN.py**: The main application that integrates the functionalities from the previous modules to process user inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "First, we will import the necessary libraries and define the global paths that will be used throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import os\n",
    "import csv\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "# Define global paths to avoid redundancy\n",
    "model_path = \"../TAKLUBAN-FILIPINO-NATIVE-LANGUAGE-PROFANE-DETECTION/LanguageIdentification/saved_model.pkl\"\n",
    "dictionary_dir = \"../TAKLUBAN-FILIPINO-NATIVE-LANGUAGE-PROFANE-DETECTION/LanguageIdentification/Dictionary\"\n",
    "output_file = \"../TAKLUBAN-FILIPINO-NATIVE-LANGUAGE-PROFANE-DETECTION/POSdata.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FNLI.py: Dictionary Generation and Model Training\n",
    "\n",
    "In this section, we will implement the functionality to generate word frequency dictionaries and train the language identification model. \n",
    "\n",
    "### 2.1 Dictionary Generation\n",
    "The `DictionaryGenerator` class will be responsible for creating frequency dictionaries for each language. It initializes noise words, loads English noise words, and removes noise words from sentences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dictionary Generation Code\n",
    "class DictionaryGenerator:\n",
    "    def __init__(self, preprocessed_dir, dictionary_dir, english_dict_path, language):\n",
    "        base_path = \"../TAKLUBAN-FILIPINO-NATIVE-LANGUAGE-PROFANE-DETECTION\"\n",
    "        self.language = language  # Add language to the instance\n",
    "        self.input_file = os.path.join(preprocessed_dir, f\"preprocessed_{language}_sentence_profane.csv\")\n",
    "        self.output_file = os.path.join(preprocessed_dir, f\"preprocessed_{language}.csv\")\n",
    "        self.english_dict_path = english_dict_path  # Corrected path for the English dictionary\n",
    "        self.preprocessed_dir = preprocessed_dir  # Ensure the directory paths are available\n",
    "        self.dictionary_dir = dictionary_dir\n",
    "        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)\n",
    "\n",
    "        # Initialize noise words for all languages\n",
    "        self.noise_words = self.initialize_noise_words()\n",
    "\n",
    "    def initialize_noise_words(self):\n",
    "        \"\"\"Initialize common noise words for Tagalog, Bikol, Cebuano, and English.\"\"\"\n",
    "        noise_words = {\n",
    "            'Tagalog': {\"ang\", \"ng\", \"sa\", \"na\", \"ay\", \"mga\", \"yung\", \"itong\", \"dito\", \"iyan\", \"kay\", \"kina\"},\n",
    "            'Bikol': {\"ang\", \"an\", \"na\", \"sa\", \"iyo\", \"kang\", \"hali\", \"ini\", \"ngani\", \"iyo\", \"baga\", \"si\", \"sinda\"},\n",
    "            'Cebuano': {\"ang\", \"sa\", \"na\", \"ug\", \"ni\", \"kay\", \"kani\", \"adto\", \"kini\", \"katong\", \"ilang\", \"iyang\"}\n",
    "        }\n",
    "        noise_words['English'] = self.load_english_noise_words()\n",
    "        self.clean_english_noise_words(noise_words)\n",
    "        return noise_words\n",
    "\n",
    "    def load_english_noise_words(self):\n",
    "        noise_words = set()\n",
    "        try:\n",
    "            with open(self.english_dict_path, 'r', encoding='utf-8') as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                next(reader)  # Skip header\n",
    "                for row in reader:\n",
    "                    if row:  # Check if the row is not empty\n",
    "                        word = row[0].strip()\n",
    "                        noise_words.add(word.lower())\n",
    "            print(f\"Loaded {len(noise_words)} English noise words.\")  # Debugging line\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file {self.english_dict_path} does not exist.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        return noise_words\n",
    "\n",
    "    def clean_english_noise_words(self, noise_words):\n",
    "        \"\"\"Remove words from English noise words that exist in any of the other three language dictionaries.\"\"\"\n",
    "        tagalog_set = noise_words.get('Tagalog', set())\n",
    "        bikol_set = noise_words.get('Bikol', set())\n",
    "        cebuano_set = noise_words.get('Cebuano', set())\n",
    "\n",
    "        common_words = (tagalog_set | bikol_set | cebuano_set) & noise_words['English']\n",
    "        noise_words['English'] = noise_words['English'] - common_words\n",
    "\n",
    "    def remove_noise(self, words, language):\n",
    "        \"\"\"Remove noise words from the list of words.\"\"\"\n",
    "        return [word for word in words if word.lower() not in self.noise_words[language.capitalize()]]\n",
    "\n",
    "    def generate_dictionary(self, language):\n",
    "        \"\"\"Generate a word frequency dictionary from preprocessed sentences, excluding words found in the English dictionary.\"\"\"\n",
    "        word_count = Counter()\n",
    "        preprocessed_file = os.path.join(self.preprocessed_dir, f\"preprocessed_{language}_sentence_profane.csv\")\n",
    "\n",
    "        try:\n",
    "            with open(preprocessed_file, 'r', encoding='utf-8') as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                for row in reader:\n",
    "                    if row:  # Check if the row is not empty\n",
    "                        sentence = row[0]\n",
    "                        words = sentence.split()\n",
    "                        cleaned_words = [word for word in self.remove_noise(words, language)\n",
    "                                         if word.lower() not in self.noise_words['English']]\n",
    "                        word_count.update(cleaned_words)\n",
    "\n",
    "            self.save_dictionary(word_count, language)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file {preprocessed_file} does not exist.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def save_dictionary(self, word_count, language):\n",
    "        \"\"\"Save the word frequency dictionary to a CSV file.\"\"\"\n",
    "        dict_file = os.path.join(self.dictionary_dir, f\"{language}_dictionary.csv\")\n",
    "        with open(dict_file, 'w', newline='', encoding='utf-8') as dict_file:\n",
    "            writer = csv.writer(dict_file)\n",
    "            writer.writerow(['word', 'frequency'])\n",
    "            for word, freq in sorted(word_count.items()):\n",
    "                writer.writerow([word, freq])\n",
    "        print(f\"Dictionary saved at {dict_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Training\n",
    "The `ModelTraining` class will be responsible for training the language identification model using the generated dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Training Code\n",
    "class ModelTraining:\n",
    "    \"\"\"This class is responsible for training the language identification model.\"\"\"\n",
    "    \n",
    "    def __init__(self, dictionary_dir):\n",
    "        self.dictionary_dir = dictionary_dir\n",
    "        self.word_frequencies = self.load_dictionaries()\n",
    "\n",
    "    def load_dictionaries(self):\n",
    "        frequencies = {}\n",
    "        for language in ['tagalog', 'bikol', 'cebuano']:\n",
    "            dict_file = os.path.join(self.dictionary_dir, f\"{language}_dictionary.csv\")\n",
    "            if os.path.exists(dict_file):\n",
    "                df = pd.read_csv(dict_file)\n",
    "                frequencies[language] = dict(zip(df['word'], df['frequency']))\n",
    "                print(f\"Loaded {language} dictionary with {len(frequencies[language])} entries.\")\n",
    "            else:\n",
    "                print(f\"Dictionary file for {language} not found.\")\n",
    "        return frequencies\n",
    "\n",
    "    def train_model(self):\n",
    "        # Prepare training data\n",
    "        data = []\n",
    "        labels = []\n",
    "        for language, word_freq in self.word_frequencies.items():\n",
    "            for word, freq in word_freq.items():\n",
    "                if freq > 0 and isinstance(word, str):\n",
    "                    data.extend([word] * freq)\n",
    "                    labels.extend([language] * freq)\n",
    "\n",
    "        if not data or not labels:\n",
    "            raise ValueError(\"Training data or labels are empty. Please check your dictionary files.\")\n",
    "\n",
    "        # Split the data (60% training, 30% validation, 10% testing)\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.40, random_state=50)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.25, random_state=50)\n",
    "\n",
    "        print(f\"Train size: {len(X_train)}, Validation size: {len(X_val)}, Test size: {len(X_test)}\")\n",
    "\n",
    "        # Create a pipeline with TfidfVectorizer for N-gram extraction and MultinomialNB with Laplace smoothing\n",
    "        pipeline = make_pipeline(TfidfVectorizer(ngram_range=(1, 3)), MultinomialNB(alpha=1.0))\n",
    "\n",
    "        # Hyperparameter tuning using GridSearchCV\n",
    "        param_grid = {\n",
    "            'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "            'multinomialnb__alpha': [0.1, 0.5, 1.0]\n",
    "        }\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        model = grid_search.best_estimator_\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Save the trained model to a file\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"Model saved at {model_path}\")\n",
    "\n",
    "        return model, X_test, y_test\n",
    "\n",
    "class LanguageIdentification:\n",
    "    \"\"\"This class is responsible for predicting the language based on a pre-trained model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, X_test, y_test):\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def predict_language(self, sentence):\n",
    "        return self.model.predict([sentence])[0]\n",
    "\n",
    "    def determine_language(self, sentences):\n",
    "        language_counter = Counter()\n",
    "        for sentence in sentences:\n",
    "            dominant_language = self.predict_language(sentence)\n",
    "            language_counter[dominant_language] += 1\n",
    "        return language_counter.most_common(1)[0][0] if language_counter else None\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the model and calculate accuracy, precision, recall, and F1 score.\"\"\"\n",
    "        predictions = [self.predict_language(sentence) for sentence in self.X_test]\n",
    "\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        precision = precision_score(self.y_test, predictions, average='weighted', zero_division=0)\n",
    "        recall = recall_score(self.y_test, predictions, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(self.y_test, predictions, average='weighted', zero_division=0)\n",
    "\n",
    "        return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. POSTagger.py: Part-of-Speech Tagging\n",
    "\n",
    "In this section, we will implement the functionality for Part-of-Speech (POS) tagging using the Stanford POS Tagger. The `POSTagger` class will serve as the interface for tagging sentences in the supported languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: POSTagger Code\n",
    "class StanfordPOSTaggerWrapper:\n",
    "    def __init__(self, language):\n",
    "        base_path = \"../TAKLUBAN-FILIPINO-NATIVE-LANGUAGE-PROFANE-DETECTION\"\n",
    "        self.input_file = f\"{base_path}/Results/preprocessed/preprocessed_{language}_sentence_profane.csv\"\n",
    "        self.output_file = f\"{base_path}/Results/DATASETOFREGEX/Tagged_{language}.csv\"\n",
    "\n",
    "        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)\n",
    "\n",
    "        self.data = pd.read_csv(self.input_file, names=['sentence', 'label'])\n",
    "        print(f\"Loaded preprocessed data for {language}. Number of sentences: {len(self.data)}\")\n",
    "\n",
    "        self.tagger = StanfordPOSTagger(\n",
    "            model_filename='Modules/FSPOST/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger',\n",
    "            path_to_jar='Modules/FSPOST/stanford-postagger-full-2020-11-17/stanford-postagger.jar',\n",
    "            java_options='-mx6144m'  # Set maximum Java heap size to 6 GB\n",
    "        )\n",
    "\n",
    "    def pos_tag_text(self, text):\n",
    "        try:\n",
    "            tokens = text.split()\n",
    "            pos_tags = self.tagger.tag(tokens)\n",
    "            return ' '.join([f\"{word}|{tag}\" for word, tag in pos_tags])\n",
    "        except Exception as e:\n",
    "            print(f\"Error during POS tagging: {e}\")\n",
    "            return text\n",
    "\n",
    "    def pos_tag_sentences(self, batch_size=10):\n",
    "        try:\n",
    "            for i in range(0, len(self.data), batch_size):\n",
    "                batch = self.data.iloc[i:i + batch_size].copy()\n",
    "                batch['pos_tagged'] = batch['sentence'].apply(self.pos_tag_text)\n",
    "\n",
    "                batch[['pos_tagged', 'label']].to_csv(self.output_file, mode='a', index=False, header=(i == 0))\n",
    "                print(f\"Processed batch {i//batch_size + 1} of {len(self.data) // batch_size + 1}\")\n",
    "\n",
    "            print(f\"POS tagging complete. Results saved to {self.output_file}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during POS tagging: {e}\")\n",
    "\n",
    "class POSTagger:\n",
    "    def __init__(self, language):\n",
    "        self.language = language.lower()\n",
    "        self.output_file = 'POSTagging/POSTAGGER/POSData.csv'\n",
    "        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)\n",
    "\n",
    "        if self.language in ['tagalog', 'cebuano', 'bikol']:\n",
    "            self.stanford_tagger = StanfordPOSTaggerWrapper(language)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_token(token):\n",
    "        token = token.lstrip('/')\n",
    "        if '|' not in token:\n",
    "            print(f\"Invalid token format, no '|': {token}\")\n",
    "            return token\n",
    "        word, tag = map(str.strip, token.split('|', 1))\n",
    "        return f\"{word}|{tag}\"\n",
    "\n",
    "    def language_rules(self, token):\n",
    "        token = self.clean_token(token)\n",
    "        word, current_tag = token.split('|', 1)\n",
    "\n",
    "        patterns = {\n",
    "            'cebuano': {\n",
    "                'VB': r'\\b(mag|nag|mi|mo|mu|mang|manag|man)[a-zA-Z]+\\b',\n",
    "                'NNC': r'\\b([a-zA-Z]+on|[a-zA-Z]+an)\\b',\n",
    "                'NNCA': r'\\b(ka|pang)[a-zA-Z]+an\\b',\n",
    "                'NNPL': r'\\bmga\\s+[a-zA-Z]+\\b',\n",
    "                'JJD': r'\\b(ma|ka)[a-zA-Z]+an\\b',\n",
    "                'JJCM': r'\\bmas\\s+[a-zA-Z]+\\b',\n",
    "                'PRP': r'\\bako|ikaw|siya|kami|kita|sila\\b',\n",
    "                'DT': r'\\bang|bang|mga\\b',\n",
    "                'CCP': r'\\bug|o|kundi\\b',\n",
    "            },\n",
    "            'bikol': {\n",
    "                'VB': r'\\b(MA|MAG|NAG|MANG|PINAG|PA|KA)[a-zA-Z]+\\b',\n",
    "                'NN': r'\\b[a-zA-Z]+on\\b|\\b[a-zA-Z]+an\\b|\\b[a-zA-Z]+(TA|HON|LAY|LI)[a-zA-Z]*\\b',\n",
    "                'JJ': r'\\b(A|KA|MALA)[a-zA-Z]+on\\b|\\bPINAKA[a-zA-Z]+\\b',\n",
    "                'PRP': r'\\bAKO|IKAW|SIYA|KAMI|KITA|SINDA|NIYA|NINDA|NIATO|NATO|SAINDO\\b',\n",
    "                'DT': r'\\bANG|MGA|SI|SA|KAN|KUN\\b',\n",
    "                'RB': r'\\b(DAKUL|GAD|HALA|DAI|MAYA|SIRA|SINYA|URUG)\\b',\n",
    "                'CC': r'\\bOG|PERO|KUNDI\\b',\n",
    "                'IN': r'\\bPARA|PAAGI|ASIN|KAN\\b',\n",
    "                'CD': r'\\bSARO|DUWA|TULO|APAT|LIMA|ANOM|PITO|WALO|SIYAM|SAMPULO\\b',\n",
    "            }\n",
    "        }\n",
    "\n",
    "        language_patterns = patterns.get(self.language, {})\n",
    "        for tag, pattern in language_patterns.items():\n",
    "            if re.fullmatch(pattern, word, flags=re.IGNORECASE):\n",
    "                print(f\"Matched: {word} -> {tag}\")\n",
    "                return f\"{word}|{tag}\"\n",
    "\n",
    "        return token\n",
    "\n",
    "    def pos_tag_text(self, text):\n",
    "        stanford_tagged_text = self.stanford_tagger.pos_tag_text(text)\n",
    "        print(f\"Stanford tagged text: {stanford_tagged_text}\")\n",
    "\n",
    "        if self.language in ['cebuano', 'bikol']:\n",
    "            tokens = stanford_tagged_text.split()\n",
    "            return ' '.join([self.language_rules(token) for token in tokens])\n",
    "\n",
    "        return stanford_tagged_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TAKLUBAN.py: Main Application\n",
    "\n",
    "In this section, we will implement the main application that uses the previously defined classes to identify languages, perform POS tagging, and detect profanity in user-provided sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Annalyn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Annalyn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Annalyn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MultinomialNB from version 1.5.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Annalyn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.5.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-saved model from ../TAKLUBAN-FILIPINO-NATIVE-LANGUAGE-PROFANE-DETECTION/LanguageIdentification/saved_model.pkl.\n",
      "Welcome to Takluban Language Identifier! Enter your sentences below:\n",
      "Loaded preprocessed data for tagalog. Number of sentences: 5074\n",
      "Stanford tagged text: |NAGLALAKAD|VBTR |SIYA|PRS |NANG|RBW |MABAGAL|JJD\n",
      "SVM Prediction: Profane\n",
      "Censoring the sentence based on its length.\n",
      "Detected language: tagalog\n",
      "POS Tagged Sentence: |NAGLALAKAD|VBTR |SIYA|PRS |NANG|RBW |MABAGAL|JJD\n",
      "Censored Sentence: ********** **** **** *******\n",
      "Sentence 'naglalakad siya nang mabagal' saved with the detected language, POS tagging result, and censored sentence.\n",
      "\n",
      "Loaded preprocessed data for tagalog. Number of sentences: 5074\n",
      "Stanford tagged text: |ANG|DTC |BILIS|NNC |NIYA|PRS |TUMAKBO|VBAF\n",
      "SVM Prediction: Not Profane\n",
      "Detected language: tagalog\n",
      "POS Tagged Sentence: |ANG|DTC |BILIS|NNC |NIYA|PRS |TUMAKBO|VBAF\n",
      "Cleaned Sentence: ang bilis niya tumakbo\n",
      "Sentence 'ang bilis niya tumakbo' saved with the detected language, POS tagging result, and censored sentence.\n",
      "\n",
      "Loaded preprocessed data for tagalog. Number of sentences: 5074\n",
      "Stanford tagged text: |PUTA|NNC |ANG|DTC |HIRAP|JJD |NAMAN|RBI |NITO|PRO\n",
      "SVM Prediction: Profane\n",
      "Censoring the sentence based on its length.\n",
      "Detected language: tagalog\n",
      "POS Tagged Sentence: |PUTA|NNC |ANG|DTC |HIRAP|JJD |NAMAN|RBI |NITO|PRO\n",
      "Censored Sentence: **** *** ***** ***** ****\n",
      "Sentence 'puta ang hirap naman nito' saved with the detected language, POS tagging result, and censored sentence.\n",
      "\n",
      "Loaded preprocessed data for cebuano. Number of sentences: 5324\n",
      "Stanford tagged text: |YAWA|RBM |KA|PRS |BAI|NNC\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 94\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExiting the program.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m predicted_language, pos_tagged_sentence, censored_sentence, is_profane \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_identifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_language \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcebuano\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbikol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagalog\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected language: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_language\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 56\u001b[0m, in \u001b[0;36mprocess_sentence\u001b[1;34m(sentence, language_identifier)\u001b[0m\n\u001b[0;32m     52\u001b[0m pos_tagger \u001b[38;5;241m=\u001b[39m get_pos_tagger(predicted_language)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos_tagger \u001b[38;5;129;01mand\u001b[39;00m predicted_language \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcebuano\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbikol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagalog\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Perform initial POS tagging using Stanford POS Tagger\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     pos_tagged_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the POS tagger from POSTagger.py\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     profanity_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../TAKLUBAN-FILIPINO-NATIVE-LANGUAGE-PROFANE-DETECTION/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_language\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_trained_profane_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(profanity_model_path):\n",
      "Cell \u001b[1;32mIn[4], line 102\u001b[0m, in \u001b[0;36mPOSTagger.pos_tag_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcebuano\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbikol\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    101\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m stanford_tagged_text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stanford_tagged_text\n",
      "Cell \u001b[1;32mIn[4], line 102\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcebuano\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbikol\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    101\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m stanford_tagged_text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens])\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stanford_tagged_text\n",
      "Cell \u001b[1;32mIn[4], line 90\u001b[0m, in \u001b[0;36mPOSTagger.language_rules\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m     88\u001b[0m language_patterns \u001b[38;5;241m=\u001b[39m patterns\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage, {})\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag, pattern \u001b[38;5;129;01min\u001b[39;00m language_patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mfullmatch(pattern, word, flags\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mIGNORECASE):\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatched: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 5: TAKLUBAN Code\n",
    "# Function to check if the CSV file exists and create it if necessary\n",
    "def initialize_csv():\n",
    "    \"\"\"Ensure that the CSV file exists and has a header.\"\"\"\n",
    "    if not os.path.exists(output_file):\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Language', 'Sentence', 'POS', 'Censored Sentence'])  # Header for CSV\n",
    "\n",
    "# Centralized function for loading or training the language identification model\n",
    "def load_or_train_model():\n",
    "    \"\"\"Load or train the language identification model.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file {model_path} not found. Training a new model...\")\n",
    "        trainer = ModelTraining(dictionary_dir)\n",
    "        model, X_test, y_test = trainer.train_model()\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"Model trained and saved at {model_path}.\")\n",
    "    else:\n",
    "        print(f\"Loading pre-saved model from {model_path}.\")\n",
    "        model = joblib.load(model_path)\n",
    "        X_test, y_test = [], []  # Empty test sets as they aren't needed for prediction\n",
    "    return LanguageIdentification(model=model, X_test=X_test, y_test=y_test)\n",
    "\n",
    "# Function to get the appropriate POS tagger for the specified language\n",
    "def get_pos_tagger(language):\n",
    "    \"\"\"Return the appropriate POS tagger from POSTagger.py for the given language.\"\"\"\n",
    "    if language in ['tagalog', 'bikol', 'cebuano']:\n",
    "        return POSTagger(language)  # Create an instance of the POS tagger for the detected language\n",
    "    return None\n",
    "\n",
    "# Function for profanity detection and censorship\n",
    "def predict_and_censor(sentence, best_model, threshold=0.5):\n",
    "    \"\"\"Perform profanity detection and censorship using SVM.\"\"\"\n",
    "    probas = best_model.predict_proba([sentence])[0]  # Predict probabilities using the SVM model\n",
    "    \n",
    "    is_profane = probas[1] >= threshold  # Only classify as profane if probability is above the threshold\n",
    "    print(f\"SVM Prediction: {'Profane' if is_profane else 'Not Profane'}\")\n",
    "\n",
    "    # If SVM says the sentence is profane, censor it\n",
    "    if is_profane:\n",
    "        print(f\"Censoring the sentence based on its length.\")\n",
    "        censored_sentence = ' '.join(['*' * len(word) for word in sentence.split()])\n",
    "        return censored_sentence, True  # Return censored sentence and True to indicate it's profane\n",
    "    \n",
    "    return sentence, False  # Return the original sentence and False to indicate it's not profane\n",
    "\n",
    "# Central function for processing a sentence\n",
    "def process_sentence(sentence, language_identifier):\n",
    "    \"\"\"Process the sentence to predict the language, POS tag it using POSTagger, apply regex rules, and censor if necessary.\"\"\"\n",
    "    predicted_language = language_identifier.predict_language(sentence)\n",
    "    pos_tagger = get_pos_tagger(predicted_language)\n",
    "\n",
    "    if pos_tagger and predicted_language in ['cebuano', 'bikol', 'tagalog']:\n",
    "        # Perform initial POS tagging using Stanford POS Tagger\n",
    "        pos_tagged_sentence = pos_tagger.pos_tag_text(sentence)  # Use the POS tagger from POSTagger.py\n",
    "        \n",
    "        profanity_model_path = f'../TAKLUBAN-FILIPINO-NATIVE-LANGUAGE-PROFANE-DETECTION/{predicted_language}_trained_profane_model.pkl'\n",
    "        \n",
    "        if os.path.exists(profanity_model_path):\n",
    "            best_model = joblib.load(profanity_model_path)\n",
    "            censored_sentence, is_profane = predict_and_censor(sentence, best_model)\n",
    "        else:\n",
    "            censored_sentence = sentence\n",
    "            is_profane = False\n",
    "\n",
    "        return predicted_language, pos_tagged_sentence, censored_sentence, is_profane\n",
    "    return \"Unsupported language\", None, sentence, False\n",
    "\n",
    "def save_to_csv(language, sentence, pos_tagged, censored_sentence):\n",
    "    \"\"\"Save the language, sentence, POS tagged result, and censored sentence to a CSV file.\"\"\"\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([language, sentence, pos_tagged, censored_sentence])\n",
    "\n",
    "# Main function to run the program\n",
    "def main():\n",
    "    # Initialize CSV file and load or train the model\n",
    "    initialize_csv()\n",
    "    language_identifier = load_or_train_model()\n",
    "\n",
    "    print(\"Welcome to Takluban Language Identifier! Enter your sentences below:\")\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    while True:\n",
    "        sentence = input(\"Enter a sentence (or type 'exit' to quit): \").strip()\n",
    "\n",
    "        if sentence.lower() == 'exit':\n",
    "            print(\"Exiting the program.\")\n",
    "            break\n",
    "\n",
    "        predicted_language, pos_tagged_sentence, censored_sentence, is_profane = process_sentence(sentence, language_identifier)\n",
    "\n",
    "        if predicted_language in ['cebuano', 'bikol', 'tagalog']:\n",
    "            print(f\"Detected language: {predicted_language}\")\n",
    "            print(f\"POS Tagged Sentence: {pos_tagged_sentence}\")\n",
    "            print(f\"{'Censored Sentence' if is_profane else 'Cleaned Sentence'}: {censored_sentence}\")\n",
    "\n",
    "            save_to_csv(predicted_language, sentence, pos_tagged_sentence, censored_sentence)\n",
    "\n",
    "            # Asking the user for the true label (1 = Profane, 0 = Not Profane)\n",
    "            true_label = int(input(\"Is the sentence profane? (1 for profane, 0 for not profane): \"))\n",
    "            predictions.append(1 if is_profane else 0)\n",
    "            true_labels.append(true_label)\n",
    "\n",
    "            print(f\"Sentence '{sentence}' saved with the detected language, POS tagging result, and censored sentence.\\n\")\n",
    "        else:\n",
    "            print(f\"Unsupported language detected: {predicted_language}. No POS tagging performed.\")\n",
    "    \n",
    "    # Confusion matrix and performance metrics calculation\n",
    "    if len(predictions) > 0:\n",
    "        print(\"Confusion Matrix and Performance Metrics:\")\n",
    "        cm = confusion_matrix(true_labels, predictions)\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(true_labels, predictions))\n",
    "        \n",
    "        # Plot the confusion matrix\n",
    "        plt.figure(figsize=(6,6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=['Not Profane', 'Profane'], yticklabels=['Not Profane', 'Profane'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
